\documentclass[11pt,letterpaper]{article}
\usepackage[top=1in,bottom=1in,left=1in,right=1in]{geometry}
\usepackage{natbib}      % http://merkel.zoneo.net/Latex/natbib.php
\usepackage{palatino}
\bibpunct{(}{)}{;}{a}{,}{,}
\usepackage{chngpage}
\usepackage{stmaryrd}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{subfigure}
\usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newenvironment{itemizesquish}{\begin{list}{\labelitemi}{\setlength{\itemsep}{0em}\setlength{\labelwidth}{2em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}
\newcommand{\ignore}[1]{}
\newcommand{\transpose}{^\mathsf{T}}
\newcommand{\inner}[1]{\langle #1 \rangle}
\newcommand{\smallsec}[1]{\noindent \textbf{#1\ }}

\newcommand{\til}[1]{\widetilde{#1}}

\newcommand{\solution}[1]{{\color{Blue}[\textbf{Solution:} #1]}}
% \newcommand{\solution}[1]{}
\theoremstyle{definition}
\newtheorem{question}{Question}[section]
% \newtheorem{question}{Question}

\title{agglomerative topic model
}

\author{
Brendan O'Connor 
}

%\date{December 9, 2009}

\begin{document}
\maketitle

Goal: Find a model for which agglomerative word-level merging is doing inference for that model.

Rationale: agglomerative word-level merging is great!  Finding a model to justify it would be nice.

\section{One topic per word topic model}

Here's a topic model, in the sense it's a low-dimensional mixed-membership model for multiple multinomial groups of discrete data.  Unlike LDA it has no dirichlet priors or any priors.  For documents $d$ and token indexes $t$ and words $w \in V$ for vocabulary $V$ and topics $k \in K$,

\[ z_t \sim Disc(\theta_{d(t)}), w_t \sim \phi_{z_t} \]

using indexing notation $d(t)$ being the document at token $t$ (where token positions are unique globally in the corpus).  $\theta_d$ is one doc-topic distribution and $\phi_k$ is one topic-word distribution.  Note we use $V$ to mean either a set or that set's cardinality; same notation abuse for $K$.

The model has a one topic per word constraint: every word belongs to one topic, and it cannot appear in any other.  There exists a many-to-one map for all words, $k(w)$, representing the topic that a wordtype belongs to; this means that $P(w|z=k(w))>0$ and for all other topics $j \neq k(w)$, $P(w|z=j)=0$.
This mapping can also be interpreted as a hard clustering (a partition) over the wordtypes, into $K$ clusters.

Given a mapping, inference is trivial.  $P(z_t|w_t)$ is nonzero for only when $z_t=k(w_t)$.  There are no priors on either $\theta$ or $\phi$, so their estimates are simple maximum-likelihood relative frequency calculations.  For example, $\theta_{dk}$ is the number of $z_t=k$ within $t \in d(t)$, divided by the number of tokens in $d$.

We'll use some shortcut notation.  $w_t$ is one token at position $t$.  $w_d$ are the tokens in document $d$.  $w_k$ is the ``token text'',\footnote{that Heinrich ?2007 calls it} meaning all tokens in the corpus that have topic $k$.

$\til{w}_d$ refers to a vector of wordtype counts for document $d$.  It has length $V$ (as opposed to $w_d$ which has length $n_d$, the length of the documetn).  %Finally, $\bar{w_d}$ refers to the wordtype empirical probabilities in document $d$, where each $\bar{w_d}_v = \til{w}_{dv}/n_d$ for wordtype $v$.

The same quantities also exist for token texts: $\til{w}_k$ are the wordcounts for topic $k$.  Since there are no priors we estimate $\phi_{v|k}$ as $\til{w}_{kv}/n_k$ where $n_k$ is the total number of tokens in that topic ($n_k=\sum_v \til{w}_{kv}$).

Total likelihood of the corpus, given a mapping, can be expressed in terms of documents

\[ \log P(w) = \sum_d \log P(w_d) = \sum_d \sum_v \til{w}_{dv} \log \frac{\til{w}_{kv}}{n_k} \]

You could also write it in terms of topic-texts

\[ \log P(w) = \sum_k \log P(w_k) = \sum_k \sum_v \til{w}_{kv} \log \frac{\til{w}_{kv}}{n_k} \]

As it stands, you can't actually learn anything interesting with this model.  There's no reason for co-occurring words to want to belong to the same topic.
In LDA, the Dirichlet prior on $\theta$ encourages this: it causes documents to tend to include only a subset of topics, and thus topics need to include co-occurring words in order to explain the data.

Let's abstract this into a document sparsity function, $S(\theta_d)$, which is higher if more of the probability mass of the topics in a document tend to be in a subset of topics, or tend to have a peaked distribution.  One way to define this is a Dirichlet unnorm-log-density function, as LDA uses,

\[ S(\theta_d; \alpha) = \left(\alpha - 1\right) \log \theta_{dk} \]

Of course, if we use this, $\theta_d$ is not allowed to have zeros.  We'd like to analyze this case, and we could do it by saying instead that $\theta_d$ is a latent variable that gets a posterior distribution compromising between the uniform prior $\alpha/K$ versus the empirical doc-topic distribution $\til{z}_d/n_d$, and define the sparsity measurement function instead as $S(z_d;\alpha)$; this can totally be done with a collapsed dirichlet-multinomial, but I don't know how to work it out in terms of agglomerative merging.

Here's another approach.  Let's keep with the trivial MLE estimators for $\theta$, namely that $\theta_d=\bar{z_d}$.  To promote doc-topic sparsity, impose some sort of made-up function on it ... say, a Renyi entropy.  For example, the Shannon entropy 
\[ S_1(\theta_d) = -\sum_k \theta_k \log \theta_{dk} \]
or the Renyi-0 entropy
\[ S_0(\theta_d) = -\log 1\{\theta_d>0\} = -\log ||\theta_d||_0\]
or the Renyi-2 entropy
\[ S_2(\theta_d) = -\log \sum_k \theta_{dk}^2 = -\log ||\theta_d||^2_2 \]
etc.

Then the overall optimization objective for learning a mapping $k(.)$ is
\[ F = \lambda S(\theta) + \log P(w) \]
where $S(\theta) = \sum_d S(\theta_d)$, and $\lambda$ is a hyperparameter to control the amount of doc-topic sparsity.

\section{Merging algorithm for optimization}

Initialize $K=V$ topics, with each word as its own topic.  Greedily merge pairs of topics to improve the objective.  Can the objective improvement be calculated rapidly?

Say the candidate merge pair is topics $k$ and $j$, and we're considering the improvement $F_{new}-F_{old}$.  $P_{new}-P_{old}$ in terms of topic texts has everything cancelling except the topic texts for the merge: $w_j$ and $w_k$.  Let $w(k)$ be the reverse map of $k(w)$, where $w(k)$ is the set of wordtypes with nonzero probability under topic $k$.

\begin{align}
 P_{new}-P_{old} &= \sum_v (n_{jv}+n_{kv}) \log \frac{n_v}{n_j+n_k} - 
\end{align}


\section{Justification for one topic per word}

The idea is: mixed membership models are good, but single membership models are faster.

We want to compromise LDA to make it faster.
Where do you want single membership to happen: in the doc-topic associations, or in the topic-word associations?

topic-word associations, when viewed from the word-to-topic angle, are just naturally the sparsest of those combinations.  therefore do that.

% more justification: for LDA, you can fit it to text and infer the Dirichlet concentration parameters for both $\theta$ and $\phi$.  What happens is most documents use only a few of the topics, and most topics use only a small proportion of words; and you can see this no matter how you define ``use only a few'', such as, say $x_d =$ how many topics make up the top-99\% of prob mass of a $\theta_d$; typically you get like median $x_d=4$ or something for say $K=20$.  Call this the sparsity of the topic model.

% There are two causes of this sparsity.  One is that the empirical doc-word distributions are just sparse.  So if you want a lower dimensional simplex to encompass them, you naturally get sparsity again.  Another is that the Dirichlet priors encourage it, or somehow allow it.  If you fixed your $\alpha$ this would definitely be what's happening; but it still happens when $\alpha$ is learned.  I'm still puzzled why this is so.  But it's neat.

% LDA is called a mixed-membership model because each document can belong to multiple latent components.  It can't belong to all of them --- you couldn't really learn anything interesting if that was the case --- but it belongs to a smaller subset of them.  The most extreme case is where each document only belongs to a single topic.  You can fit this model just fine---it's called a single-membership model, and sometimes called unsupervised multinomial Naive Bayes.  But it tends to work poorly; the topic-word distributions don't seem to make as much sense.


\bibliographystyle{plainnat}
\bibliography{brenocon}
\end{document}
